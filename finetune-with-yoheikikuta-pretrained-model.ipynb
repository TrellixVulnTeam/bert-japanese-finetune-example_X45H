{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ldcc-20140209.tar.gz already exists. download stopped\n",
      "./lldc already exists. extract stopped\n"
     ]
    }
   ],
   "source": [
    "from livedoor import LivedoorNewsCorpus\n",
    "\n",
    "corpus = LivedoorNewsCorpus(extract_dir='./lldc')\n",
    "corpus.download_and_extract()\n",
    "categories = corpus.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_text, all_label = corpus.get_text_and_labels()\n",
    "df = pd.DataFrame({'text': all_text, 'label': all_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining, BertForSequenceClassification\n",
    "\n",
    "# configの用意 (語彙数は30522 -> 32000に修正しておく)\n",
    "bertconfig = BertConfig.from_pretrained('bert-base-uncased',\n",
    "                                        num_labels=len(categories),\n",
    "                                        output_attentions = False,\n",
    "                                        output_hidden_states = False,\n",
    "                                       )\n",
    "bertconfig.vocab_size = 32000\n",
    "\n",
    "# BERTモデルの\"ガワ\"の用意 (全パラメーターはランダムに初期化されている)\n",
    "pretrained = BertForPreTraining(bertconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_BERT_KIKUTA = '/home/miyamonz/2019-12-12-Bert-example/model/bert-baseline/'\n",
    "BASE_CKPT = 'model.ckpt-1400000'    # 拡張子は含めない\n",
    "\n",
    "# TensorFlowモデルの重み行列を読み込む (数分程度かかる場合がある)\n",
    "pretrained.load_tf_weights(bertconfig, DIR_BERT_KIKUTA + BASE_CKPT)\n",
    "pretrained.save_pretrained(\"./tmp/\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp\n",
    "BASE_SPM = 'wiki-ja.model'\n",
    "BASE_VOCAB = 'wiki-ja.vocab'\n",
    "\n",
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(DIR_BERT_KIKUTA + BASE_SPM)\n",
    "\n",
    "#bert tokenizerのencode_plusと似たような出力になるようにする\n",
    "def spm_encode(example, max_length = 512):\n",
    "    raw_pieces  = spm.EncodeAsPieces(example)\n",
    "\n",
    "    # if input size is over max_length, truncate them\n",
    "    # Account for [CLS], [SEP] with `- 2`\n",
    "    if len(raw_pieces) > max_length-2:\n",
    "        raw_pieces = raw_pieces[:max_length-2]\n",
    "\n",
    "\n",
    "    pieces = []\n",
    "\n",
    "    # first token must be CLS\n",
    "    pieces.append(\"[CLS]\")\n",
    "\n",
    "    for piece in raw_pieces:\n",
    "        pieces.append(piece)\n",
    "\n",
    "    # last token must be SEP\n",
    "    pieces.append('[SEP]')\n",
    "\n",
    "    # convert pieces to ids\n",
    "    input_ids = [ spm.PieceToId(p) for p in pieces ]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    #fill 0 in the rest list space\n",
    "    while len(input_ids) < max_length:\n",
    "        input_ids.append(0)\n",
    "        attention_mask.append(0)\n",
    "\n",
    "    return {\n",
    "            \"input_ids\":[input_ids],\n",
    "            \"attention_mask\":[attention_mask],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def _convert_sentence_to_ids(_sentences):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in _sentences:\n",
    "        encoded_dict = spm_encode(sent)\n",
    "        _ = torch.tensor(encoded_dict['input_ids'])\n",
    "        input_ids.append(_)\n",
    "        _ = torch.tensor(encoded_dict['attention_mask'])\n",
    "        attention_masks.append(_)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return (input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factory import get_optimizers, get_dataloader\n",
    "\n",
    "#model = get_model(num_labels=len(categories))\n",
    "optimizer = get_optimizers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sentences = df.text.values\n",
    "_labels = df.label.astype('category').cat.codes\n",
    "# 文字列のラベルを数値に変換するところはpandasのcategory型を使ったが、pandas頼らなくても良い\n",
    "\n",
    "id_and_masks = _convert_sentence_to_ids(_sentences)\n",
    "labels = torch.tensor(_labels, dtype=torch.int64)\n",
    "ds_source = id_and_masks + (labels,)\n",
    "\n",
    "train_dl, valid_dl = get_dataloader(ds_source, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Training took: 0:06:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Training took: 0:06:21\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Training took: 0:06:22\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Training took: 0:06:22\n",
      "\n",
      "Running Validation...\n",
      "  Validation took: 0:00:13\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:26:18 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from fit import fit\n",
    "stats = fit(model, train_dl, valid_dl, optimizer, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.40225998480299846,\n",
       "  'Valid. Loss': 0.1971782452351338,\n",
       "  'Valid. Accur.': 0.9594594594594594,\n",
       "  'Training Time': '0:06:20',\n",
       "  'Validation Time': '0:00:13'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.1434100202865566,\n",
       "  'Valid. Loss': 0.21691012028101328,\n",
       "  'Valid. Accur.': 0.9608108108108108,\n",
       "  'Training Time': '0:06:21',\n",
       "  'Validation Time': '0:00:13'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': 0.08883859060785596,\n",
       "  'Valid. Loss': 0.19121961851377745,\n",
       "  'Valid. Accur.': 0.9689189189189189,\n",
       "  'Training Time': '0:06:22',\n",
       "  'Validation Time': '0:00:13'},\n",
       " {'epoch': 4,\n",
       "  'Training Loss': 0.06136125998416482,\n",
       "  'Valid. Loss': 0.21733691982320838,\n",
       "  'Valid. Accur.': 0.9675675675675676,\n",
       "  'Training Time': '0:06:22',\n",
       "  'Validation Time': '0:00:13'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
